---
title: "Assignment 2 - Methods 4"
author: "Laurits Lyngbaek"
date: "2025-03-18"
output: html_document
---
# Second assignment
The second assignment uses chapter 3, 5 and 6. The focus of the assignment is getting an understanding of causality.

##  Chapter 3: Causal Confussion - Regitze
**Reminder: We are tying to estimate the probability of giving birth to a boy**
I have pasted a working solution to questions 6.1-6.3 so you can continue from here:)

**3H3**
Use rbinom to simulate 10,000 replicates of 200 births. You should end up with 10,000 numbers, each one a count of boys out of 200 births. Compare the distribution of predicted numbers of boys to the actual count in the data (111 boys out of 200 births). 

```{r 3h3}
# 3H1
# Find the posterior probability of giving birth to a boy:
pacman::p_load(rethinking)
data(homeworkch3)
set.seed(1)
W <- sum(birth1) + sum(birth2)
N <- length(birth1) + length(birth2)
p_grid <-seq(from =0, to = 1, len =1000)
prob_p <- rep(1,1000)
prob_data <- dbinom(W,N,prob=p_grid)
posterior <-prob_data * prob_p
posterior <- posterior / sum(posterior)

# 3H2
# Sample probabilities from posterior distribution:
samples <- sample (p_grid, prob = posterior, size =1e4, replace =TRUE)


# 3H3
# Simulate births using sampled probabilities as simulation input, and check if they allign with real value.
simulated_births <- rbinom(n = 1e4, size = N, prob = samples)
rethinking::dens(simulated_births,show.HPDI = 0.95)
abline(v=W, col="red")
title("Simulated amount of boys in 200 births - red line is real value")

```

**3H4.**
Now compare 10,000 counts of boys from 100 simulated first borns only to the number of boys in the first births, birth1. How does the model look in this light?

```{r 3h4}

#3H4
#boys of firth birth
W_first <- sum(birth1)
N_first <- length(birth1)

#simulating
simulated_first_births <- rbinom(n = 1e4, size = N_first, prob = samples)

#plotting simulated first-births
dens(simulated_first_births, show.HPDI = 0.95, adj = 1)
abline(v = W_first, col = "red")
title("Simulated first-born boys in 100 births - red line is real value")

```


**3H5.** 
The model assumes that sex of first and second births are independent. To check this assumption, focus now on second births that followed female first borns. Compare 10,000 simulated counts of boys to only those second births that followed girls. To do this correctly, you need to count the number of first borns who were girls and simulate that many births, 10,000 times. Compare the counts of boys in your simulations to the actual observed count of boys following girls. How does the model look in this light? Any guesses what is going on in these data?

```{r}

#count first-born girls and boys after girls
N_girls_first <- sum(birth1 == 0)
W_boyaftergirl <- sum(birth1 == 0 & birth2 == 1)

#simulating second boys after girls
simulated_boy_after_girl <- rbinom(n = 1e4, size = N_girls_first, prob = samples)

#plotting
dens(simulated_boy_after_girl, show.HPDI = 0.95, adj = 1)
abline(v = W_boyaftergirl, col = "red")
title("Simulated second-born boys after girls - red line is real value")

```

*Answer*: The distribution of boy being born aftern first-born girls in the simulation is much lower than the actual observation. This could mean that the order in which boys and girls are being born are not indipendent from each other, but a boy being born after a girl is more probable than just a boy being born as the first-born.

## Chapter 5: Spurrious Correlations
Start of by checking out all the spurrious correlations that exists in the world.
Some of these can be seen on this wonderful website: https://www.tylervigen.com/spurious/random
All the medium questions are only asking you to explain a solution with words, but feel free to simulate the data and prove the concepts.


**5M1**. - Katrine
Invent your own example of a spurious correlation. An outcome variable should be correlated
with both predictor variables. But when both predictors are entered in the same model, the correlation
between the outcome and one of the predictors should mostly vanish (or at least be greatly reduced).

Number of people with bad breath -> number of burn victims int he hospital

But in reality
Number of people drinking coffee -> number of burn victims in the hospital
Number of people drinking coffee -> number of people with bad breath

 
**5M2**. - Rahel
Invent your own example of a masked relationship. An outcome variable should be correlated
with both predictor variables, but in opposite directions. And the two predictor variables should be
correlated with one another.

Number of sunny days -> number of flowers on the field
Number of windy days -> number of flowers on the field
Number of sunny days <-> number of windy days


**5M3**. - Regitze
It is sometimes observed that the best predictor of fire risk is the presence of firefighters—
States and localities with many firefighters also have more fires. Presumably firefighters do not cause
fires. Nevertheless, this is not a spurious correlation. Instead fires cause firefighters. Consider the
same reversal of causal inference in the context of the divorce and marriage data. How might a high
divorce rate cause a higher marriage rate? Can you think of a way to evaluate this relationship, using
multiple regression

**5M5**. - Sári
One way to reason through multiple causation hypotheses is to imagine detailed mechanisms
through which predictor variables may influence outcomes. For example, it is sometimes argued that
the price of gasoline (predictor variable) is positively associated with lower obesity rates (outcome
variable). However, there are at least two important mechanisms by which the price of gas could
reduce obesity. First, it could lead to less driving and therefore more exercise. Second, it could lead to
less driving, which leads to less eating out, which leads to less consumption of huge restaurant meals.
Can you outline one or more multiple regressions that address these two mechanisms? Assume you
can have any predictor data you need.

*Answer:* First we create notations for the predictors and outcomes:
- Price of Gasoline (predictor): G
- Amount of Driving (confound): D
- Amount of Excercise (confound): E
- Amount of Restaurant Meals (confound): M
- Obesity rate (outcome): O

We have two ways that the predictors influence each other and thus the outcome, which can be our two multiple regressions:
- Path 1:  G -> D -> E -> O
- Path 2: G -> D -> M -> O

```{r 5M5}

library(dagitty)

dag_5m5 <- dagitty("dag {
                     G -> D
                     D -> E
                     D -> M
                     E -> O
                     M -> O
                     }")

plot(dag_5m5)

```


From this we can conclude:

- If gasoline is cheaper, people drive more, so they exercise less, so they are more obese.
- If gasoline is cheaper, people drive more, so they eat out more, so they are more obese.

Looking at the two paths we can see that driving has the same explanatory value in both, so we can just create a multiple regression with the outcome being obesity rate, an intercept, a beta estimate multiplied by a measure of how much people exercise by walking, a beta estimate multiplied by a measure of how much people eat out, and a beta estimate multiplied by the price of gasoline. By looking at the mean estimates of the beta parameters we can check which predictor describes the correlation better. We can also create a full-model, combining beta estimates of gas prices, eating out and exercise, so we can see if it is really through these paths that obesity is mediated.

## Chapter 5: Foxes and Pack Sizes - Rahel
All five exercises below use the same data, data(foxes) (part of rethinking).84 The urban fox (Vulpes vulpes) is a successful exploiter of human habitat. Since urban foxes move in packs and defend territories, data on habitat quality and population density is also included. The data frame has five columns:
(1) group: Number of the social group the individual fox belongs to
(2) avgfood: The average amount of food available in the territory
(3) groupsize: The number of foxes in the social group
(4) area: Size of the territory
(5) weight: Body weight of the individual fox

**5H1.** 
Fit two bivariate Gaussian regressions, using quap: (1) body weight as a linear function of territory size (area), and (2) body weight as a linear function of groupsize. Plot the results of these regressions, displaying the MAP regression line and the 95% interval of the mean. Is either variable important for predicting fox body weight?
```{r 5H1}

#loading data
library(rethinking)
data("foxes")
data <- foxes

data$W <- scale(data$weight)
data$T <- scale(data$area)
data$G <- scale(data$group)


#modelling weight from territory size
model_5h1_1 <- quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- a + bT*T,
    a ~ dnorm(0, 0.2),
    bT ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = data
)

#modelling weight from groupsize
model_5h1_2 <- quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- a + bG*G,
    a ~ dnorm(0, 0.2),
    bG ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = data
)

#plotting
xseq <- seq(from=-2, to=2, length.out=30) #checked range of weights
mu <- link( model_5h1_1, data=list(T=xseq))
mu_mean <- apply(mu,2,mean)
mu_PI <- apply(mu,2,PI)
plot( W ~ T , data=data )
lines( xseq , mu_mean , lwd=2 )
shade( mu_PI , xseq )

xseq <- seq(from=-2, to=2, length.out=30) #checked range of weights
mu <- link( model_5h1_2, data=list(G=xseq))
mu_mean <- apply(mu,2,mean)
mu_PI <- apply(mu,2,PI)
plot( W ~ G , data=data )
lines( xseq , mu_mean , lwd=2 )
shade( mu_PI , xseq )

```

**5H2.**
Now fit a multiple linear regression with weight as the outcome and both area and groupsize as predictor variables. Plot the predictions of the model for each predictor, holding the other predictor constant at its mean. What does this model say about the importance of each variable? Why do you get different results than you got in the exercise just above?
```{r}

#modelling with both predictors
model_5h2 <- quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- a + bG*G + bT*T,
    a ~ dnorm(0, 0.2),
    bG ~ dnorm(0, 0.5),
    bT ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = data
)

#plotting for each predictor
xseq <- seq( from=-2 , to=2 , length.out=30 )
mu <- link( model_5h2 , data=data.frame( G=xseq , T=0 ) )
mu_mean <- apply(mu,2,mean)
mu_PI <- apply(mu,2,PI)
plot( NULL , xlim=range(data$G) , ylim=range(-1.5, 1.5) )
lines( xseq , mu_mean , lwd=2 )
shade( mu_PI , xseq )

#plotting for each predictor
xseq <- seq( from=-2 , to=2 , length.out=30 )
mu <- link( model_5h2 , data=data.frame( T=xseq , G=0 ) )
mu_mean <- apply(mu,2,mean)
mu_PI <- apply(mu,2,PI)
plot( NULL , xlim=range(data$T) , ylim=range(-1.5, 1.5) )
lines( xseq , mu_mean , lwd=2 )
shade( mu_PI , xseq )

```

**5H3.**
Finally, consider the avgfood variable. Fit two more multiple regressions: (1) body weight as an additive function of avgfood and groupsize, and (2) body weight as an additive function of all three variables, avgfood and groupsize and area. Compare the results of these models to the previous models you’ve fit, in the first two exercises. (a) Is avgfood or area a better predictor of body weight? If you had to choose one or the other to include in a model, which would it be? Support your assessment with any tables or plots you choose. (b) When both avgfood or area are in the same model, their effects are reduced (closer to zero) and their standard errors are larger than when they are included in separate models. Can you explain this result?

```{r}

data$F <- scale(data$avgfood)

#fitting models
model_5h3_1 <- quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- a + bG*G + bF*F,
    a ~ dnorm(0, 0.2),
    bG ~ dnorm(0, 0.5),
    bF ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = data
)

model_5h3_2 <- quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- a + bG*G + bT*T + bF*F,
    a ~ dnorm(0, 0.2),
    bG ~ dnorm(0, 0.5),
    bT ~ dnorm(0, 0.5),
    bF ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = data
)

precis(model_5h1_1)
precis(model_5h1_2)
precis(model_5h2)
precis(model_5h3_1)
precis(model_5h3_2)

#question b not true for me?!?

```


**Defining our theory with explicit DAGs** - Katrine
Assume this DAG as an causal explanation of fox weight:

```{r}
pacman::p_load(dagitty,
               ggdag)
dag <- dagitty('dag {
A[pos="1.000,0.500"]
F[pos="0.000,0.000"]
G[pos="2.000,0.000"]
W[pos="1.000,-0.500"]
A -> F
F -> G
F -> W
G -> W
}')

# Plot the DAG
ggdag(dag, layout = "circle")+
  theme_dag()

```
where A is area, F is avgfood,G is groupsize, and W is weight. 

**Using what you know about DAGs from chapter 5 and 6, solve the following three questions:**

1) Estimate the total causal influence of A on F. What effect would increasing the area of a territory have on the amount of food inside of it?

```{r}

adjustmentSets(dag, exposure = "A", outcome = "F")

data$A <- data$T

#no open doors?
model_1 <- quap(
  alist(
    F ~ dnorm(mu, sigma),
    mu <- a + bA*A,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = data
)

precis(model_1)

```

2) Infer the **total** causal effect of adding food F to a territory on the weight W of foxes. Can you calculate the causal effect by simulating an intervention on food?

```{r}

impliedConditionalIndependencies(dag)

model_2 <- quap(
  alist(
    #food model -> predicting food from area size
    F ~ dnorm(mu_F, sigma_F),
    mu_F <- aF + bA * A,
    aF ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    sigma_F ~ dexp(1),
    
    #group size model -> predicting group size from food
    G ~ dnorm(mu_G, sigma_G),
    mu_G <- aG + bFG * F,
    aG ~ dnorm(0, 0.2),
    bFG ~ dnorm(0, 0.5),
    sigma_G ~ dexp(1),
    
    #weight model -> predicting weight from food and group size
    W ~ dnorm(mu_W, sigma_W),
    mu_W <- aW + bF * F + bG * G,
    aW ~ dnorm(0, 0.2),
    bF ~ dnorm(0, 0.5),
    bG ~ dnorm(0, 0.5),
    sigma_W ~ dexp(1)
  ), data = data
)

precis(model_2)

#total effect -> bF + bFG * bG

```

3) Infer the **direct** causal effect of adding food F to a territory on the weight W of foxes. In light of your estimates from this problem and the previous one, what do you think is going on with these foxes? 

```{r}

#bF from previous model?

```

## Chapter 6: Investigating the Waffles and Divorces - Sári
**6H1**. 
Use the Waffle House data, data(WaffleDivorce), to find the total causal influence of number of Waffle Houses on divorce rate. Justify your model or models with a causal graph.

```{r 6H1}

#setting up data
library(rethinking)
data("WaffleDivorce")
data <- WaffleDivorce
data$S <- scale(data$South)
data$A <- scale(data$MedianAgeMarriage)
data$D <- scale(data$Divorce)
data$M <- scale(data$Marriage)
data$W <- scale(data$WaffleHouses)

#causal graph
library(dagitty)
library(ggplot2)
print("S: if a state is southern, A: median age at marriage, D: divorce rate, M: marriage rate, W: number of Waffle Houses")
dag_6h1 <- dagitty("dag {
                   A -> D
                   A -> M -> D
                   A <- S -> M
                   S -> W -> D
                   }")
coordinates(dag_6h1) <- list(
  x = c(A = 1, S = 1, M = 2, W = 3, D = 3),
  y = c(A = 3, S = 1, M = 2, W = 1, D = 3)
)

ggplot(dag_6h1,
       aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(size = 20, shape = 20, color = "orange2") +
  geom_dag_text(color = "red4") +
  geom_dag_edges() +
  theme_dag()

#modelling
##conditioning on south as it is the open only back door on divorce reate that is not connected to the number of waffle houses
model_6h1 <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bS*S + bW*W,
    a ~ dnorm(0, 0.2),
    bS ~ dnorm(0, 0.5),
    bW ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = data
)

precis(model_6h1)

#getting summary of model
summary_6h1 <- precis(model_6h1, depth = 2) %>% as.data.frame()
summary_6h1$parameter <- rownames(summary_6h1)

#plotting
ggplot(summary_6h1, aes(x = mean, y = reorder(parameter, mean))) +
  geom_point(size = 3, color = "orange2") + 
  geom_errorbarh(aes(xmin = `5.5%`, xmax = `94.5%`), height = 0.2, color = "red4") +
  theme_minimal() +
  labs(
    title = "Posterior Estimates with 89% Credible Intervals",
    subtitle = "Predicting Divorce Rate from South and Number of Waffle Houses",
    x = "Parameter Estimate",
    y = "Parameters"
  )
  
```

*Answer*: We can see that with a model conditioned on if a state is in the south of the US or not, we can see that the number of Waffle Houses contains no information on the divorce rate of the sate, that the southernity does not already explain. From this we could conclude that it is just a spurious correlation between the number of Waffle Houses in a state and divorce rate, that is mediated by the connection of the location of the state (southern states having both more Waffle Houses and more divorces). 

**6H2**. 
Build a series of models to test the implied conditional independencies of the causal graph you used in the previous problem. If any of the tests fail, how do you think the graph needs to be amended? Does the graph need more or fewer arrows? Feel free to nominate variables that aren't in the data.

```{r}

impliedConditionalIndependencies(dag_6h1)

#testing first one - A _||_ W | S
model_6h2_1 <- quap(
  alist(
    A ~ dnorm(mu, sigma),
    mu <- a + bS*S + bW*W,
    a ~ dnorm(0, 0.2),
    bS ~ dnorm(0, 0.5),
    bW ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = data
)

precis(model_6h2_1)

#getting summary of model
summary_6h2_1 <- precis(model_6h2_1, depth = 2) %>% as.data.frame()
summary_6h2_1$parameter <- rownames(summary_6h2_1)

#plotting
plot1 <- ggplot(summary_6h2_1, aes(x = mean, y = reorder(parameter, mean))) +
  geom_point(size = 3, color = "orange2") + 
  geom_errorbarh(aes(xmin = `5.5%`, xmax = `94.5%`), height = 0.2, color = "red4") +
  theme_minimal() +
  labs(
    title = "Posterior Estimates with 89% Credible Intervals",
    subtitle = "Predicting Median Age at Marriage from South and Number of Waffle Houses",
    x = "Parameter Estimate",
    y = "Parameters"
  )

#testing second one - D _||_ S | A, M, W
model_6h2_2 <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bS*S + bW*W + bA*A + bM*M,
    a ~ dnorm(0, 0.2),
    bS ~ dnorm(0, 0.5),
    bW ~ dnorm(0, 0.5),
    bA ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = data
)
precis(model_6h2_2)

#getting summary of model
summary_6h2_2 <- precis(model_6h2_2, depth = 2) %>% as.data.frame()
summary_6h2_2$parameter <- rownames(summary_6h2_2)

#plotting
plot2 <- ggplot(summary_6h2_2, aes(x = mean, y = reorder(parameter, mean))) +
  geom_point(size = 3, color = "orange2") + 
  geom_errorbarh(aes(xmin = `5.5%`, xmax = `94.5%`), height = 0.2, color = "red4") +
  theme_minimal() +
  labs(
    title = "Posterior Estimates with 89% Credible Intervals",
    subtitle = "Predicting Divorce Rate from South, Medium Age at Marriage, Marriage Rate and Number of Waffle Houses",
    x = "Parameter Estimate",
    y = "Parameters"
  )

#testing third one - M _||_ W | S
model_6h2_3 <- quap(
  alist(
    M ~ dnorm(mu, sigma),
    mu <- a + bS*S + bW*W,
    a ~ dnorm(0, 0.2),
    bS ~ dnorm(0, 0.5),
    bW ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = data
)
precis(model_6h2_3)

#getting summary of model
summary_6h2_3 <- precis(model_6h2_3, depth = 2) %>% as.data.frame()
summary_6h2_3$parameter <- rownames(summary_6h2_3)

#plotting
plot3 <- ggplot(summary_6h2_3, aes(x = mean, y = reorder(parameter, mean))) +
  geom_point(size = 3, color = "orange2") + 
  geom_errorbarh(aes(xmin = `5.5%`, xmax = `94.5%`), height = 0.2, color = "red4") +
  theme_minimal() +
  labs(
    title = "Posterior Estimates with 89% Credible Intervals",
    subtitle = "Predicting Marriage Rate from South and Number of Waffle Houses",
    x = "Parameter Estimate",
    y = "Parameters"
  )

library(gridExtra)
grid.arrange(plot1, plot2, plot3, nrow = 3)


```

*Answer*:
- From the first test we can see that median age and the number of Waffle Houses are independent form each other when controlling for the southernity of the state.
- From the second test we can see that even when controlling for median age, number of Waffle Houses and marriage rate, sourthernity still has a significant effect on divorce reate. This suggest that we should have a direct relationship between S and D in our DAG graph.
- from the third tast we can- see that marriage rate and the number of Waffle Houses are independent from each other when controllin for the southernity of the state.

